{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"bert_finetune.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"idBW9olEPYGR"},"source":["# **Setup**"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0uNIu6ih72wP","executionInfo":{"elapsed":10324,"status":"ok","timestamp":1626150194878,"user":{"displayName":"漏气了","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64","userId":"08159925913877395493"},"user_tz":420},"outputId":"eb0684f0-ea02-4d94-fe56-77e12015655c"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 12.4MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 33.1MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 36.8MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3TIEdUY523BF","executionInfo":{"elapsed":795,"status":"ok","timestamp":1626150199164,"user":{"displayName":"漏气了","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64","userId":"08159925913877395493"},"user_tz":420},"outputId":"df903cc9-6e17-423d-8e10-5e7df02720e5"},"source":["!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-07-13 04:23:18--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.79.142\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.79.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 231508 (226K) [text/plain]\n","Saving to: ‘bert-base-uncased-vocab.txt’\n","\n","bert-base-uncased-v 100%[===================>] 226.08K  1.11MB/s    in 0.2s    \n","\n","2021-07-13 04:23:18 (1.11 MB/s) - ‘bert-base-uncased-vocab.txt’ saved [231508/231508]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oXmZ6ISegREN","executionInfo":{"elapsed":18322,"status":"ok","timestamp":1626150220550,"user":{"displayName":"漏气了","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64","userId":"08159925913877395493"},"user_tz":420},"outputId":"30caf8a0-5cc4-41fc-cf29-6f851d0d1724"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path=\"/content/drive/My Drive/Colab Notebooks/\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PwRIaL1U7Ptp"},"source":["import os\n","import re\n","import json\n","import string\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from transformers import TFBertModel\n","from tokenizers import BertWordPieceTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QOtrxNVY7Ptq","executionInfo":{"elapsed":3909,"status":"ok","timestamp":1626150233185,"user":{"displayName":"漏气了","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64","userId":"08159925913877395493"},"user_tz":420},"outputId":"5dfe97e8-b9b6-4e37-845d-523f2f698e99"},"source":["tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n","\n","train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n","eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n","\n","train_path = keras.utils.get_file(\"train.json\", train_data_url)\n","eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)\n","\n","max_len = 400\n","\n","with open(train_path) as f:\n","    raw_train_data = json.load(f)\n","\n","with open(eval_path) as f:\n","    raw_eval_data = json.load(f)\n","\n","train_all = False\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n","42131456/42123633 [==============================] - 1s 0us/step\n","Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n","4374528/4370528 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tMrP0HK-OXpO"},"source":["# **Preprocessing data**"]},{"cell_type":"code","metadata":{"id":"FqW9F17QHD5c"},"source":["def convert_squad2data(raw_data):\n","    squad_data = []\n","    for item in raw_data[\"data\"]:\n","        for para in item[\"paragraphs\"]:\n","            context = para[\"context\"]\n","            for qa in para[\"qas\"]:\n","                question = qa[\"question\"]\n","                has_answer = True if qa['answers'] else False\n","                if has_answer:\n","                    answer_text = qa[\"answers\"][0][\"text\"]\n","                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n","                    answer_start_idx = qa[\"answers\"][0][\"answer_start\"]\n","                    squad_sample = SquadDataSample(question, context, answer_start_idx, answer_text, all_answers)\n","                else:\n","                    squad_sample = SquadDataSample(question, context)\n","                squad_sample.preprocess()\n","                squad_data.append(squad_sample)\n","    return squad_data\n","\n","\n","def create_input_dataset(squad_samples):\n","    dataset_dict = {\n","        \"input_ids\": [],\n","        \"token_type_ids\": [],\n","        \"attention_mask\": [],\n","        \"start_token_idx\": [],\n","        \"end_token_idx\": [],\n","    }\n","    for sample in squad_samples:\n","        if not sample.skip:\n","            for key in dataset_dict:\n","                dataset_dict[key].append(getattr(sample, key))\n","    for key in dataset_dict:\n","        dataset_dict[key] = np.array(dataset_dict[key])\n","\n","    x = [\n","        dataset_dict[\"input_ids\"],\n","        dataset_dict[\"token_type_ids\"],\n","        dataset_dict[\"attention_mask\"],\n","    ]\n","    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n","    return x, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGzwg_al7Pts"},"source":["class SquadDataSample:\n","    def __init__(self, question, context, answer_start_idx=0, answer_text=None, all_answers=None):\n","        self.question = question\n","        self.context = context\n","        self.answer_start_idx = answer_start_idx\n","        self.answer_text = answer_text\n","        self.all_answers = all_answers\n","        self.skip = False\n","\n","    def preprocess(self):\n","        context = self.context\n","        question = self.question\n","        answer_text = self.answer_text\n","        answer_start_idx = self.answer_start_idx\n","\n","        context = \" \".join(str(context).split())\n","        question = \" \".join(str(question).split())\n","\n","        tokenized_context, tokenized_question = self.tokenize_data(context, question)\n","\n","        if self.answer_text is not None:\n","            answer = \" \".join(str(answer_text).split())\n","            answer_end_idx = answer_start_idx + len(answer)\n","            if answer_end_idx >= len(context):\n","                self.skip = True\n","                return\n","            is_char_in_ans = [0] * len(context)\n","            for idx in range(answer_start_idx, answer_end_idx):\n","                is_char_in_ans[idx] = 1\n","            answer_token_idx = []\n","            for idx, (start, end) in enumerate(tokenized_context.offsets):\n","                if sum(is_char_in_ans[start:end]) > 0:\n","                    answer_token_idx.append(idx)\n","            if len(answer_token_idx) == 0:\n","                self.skip = True\n","                return\n","            start_token_idx = answer_token_idx[0]\n","            end_token_idx = answer_token_idx[-1]\n","        else:\n","            start_token_idx = 0\n","            end_token_idx = 0\n","\n","        attention_mask, input_ids, token_type_ids = self.create_sample_features(tokenized_context, tokenized_question)\n","\n","        self.input_ids = input_ids\n","        self.token_type_ids = token_type_ids\n","        self.attention_mask = attention_mask\n","        self.start_token_idx = start_token_idx\n","        self.end_token_idx = end_token_idx\n","        self.context_token_to_char = tokenized_context.offsets\n","\n","    def create_sample_features(self, tokenized_context, tokenized_question):\n","        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n","        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n","        attention_mask = [1] * len(input_ids)\n","        padding_length = max_len - len(input_ids)\n","        if padding_length > 0:\n","            input_ids = input_ids + ([0] * padding_length)\n","            attention_mask = attention_mask + ([0] * padding_length)\n","            token_type_ids = token_type_ids + ([0] * padding_length)\n","        elif padding_length < 0:\n","            self.skip = True\n","        return attention_mask, input_ids, token_type_ids\n","\n","    def tokenize_data(self, context, question):\n","        tokenized_context = tokenizer.encode(context)\n","        tokenized_question = tokenizer.encode(question)\n","        return tokenized_context, tokenized_question\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUImCGaxh7NS","executionInfo":{"elapsed":79872,"status":"ok","timestamp":1626150438415,"user":{"displayName":"漏气了","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64","userId":"08159925913877395493"},"user_tz":420},"outputId":"23fc0f72-2b04-48a2-8a57-e2322c8c8ac0"},"source":["train_squad_samples = convert_squad2data(raw_train_data)\n","eval_squad_samples = convert_squad2data(raw_eval_data)\n","\n","if train_all:\n","    x_train, y_train = create_input_dataset(train_squad_samples)\n","    x_eval, y_eval = create_input_dataset(eval_squad_samples)\n","    x_test, y_test = x_eval, y_eval\n","\n","    print(\"num of train data: \" + str(len(train_squad_samples)))\n","    print(\"num of eval data: \" + str(len(eval_squad_samples)))\n","else:\n","    n_train = 30000\n","    n_eval = 5000\n","    n_test = 600\n","    x_train, y_train = create_input_dataset(train_squad_samples[:n_train])\n","    x_eval, y_eval = create_input_dataset(eval_squad_samples[:n_eval])\n","    x_test, y_test = create_input_dataset(eval_squad_samples[n_eval:n_eval + n_test])\n","\n","    print(\"num of train data used for train: \" + str(len(train_squad_samples[:n_train])))\n","    print(\"num of eval data used for eval: \" + str(len(eval_squad_samples[:n_eval])))\n","    print(\"num of train data used for test: \" + str(len(eval_squad_samples[n_eval:n_eval + n_test])))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["num of train data used for train: 30000\n","num of eval data used for eval: 5000\n","num of train data used for test: 600\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y7jGBLKNOkGm"},"source":["# **Create model**"]},{"cell_type":"code","metadata":{"id":"GEJAQAIb7Pt5"},"source":["def create_model(learning_rate=5e-5, decay=1e-6):\n","    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n","\n","    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n","    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n","    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n","    embedding = bert_model.bert(\n","        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n","    )[0]\n","\n","    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n","    start_logits = layers.Flatten()(start_logits)\n","\n","    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n","    end_logits = layers.Flatten()(end_logits)\n","\n","    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n","    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n","\n","    model = keras.Model(\n","        inputs=[input_ids, token_type_ids, attention_mask],\n","        outputs=[start_probs, end_probs],\n","    )\n","    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","    optimizer = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n","    model.compile(optimizer=optimizer, loss=[loss, loss], metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSSKo6jG7Pt8"},"source":["# **Train and Evaluate model**\n"]},{"cell_type":"markdown","metadata":{"id":"EPpE5JN6PBKA"},"source":["Training model"]},{"cell_type":"code","metadata":{"id":"M7_W7Nfv7Pt6"},"source":["# model = create_model(learning_rate=5e-5, decay=1e-6)\n","# model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d7Pei6tN7Pt8","outputId":"ac5d0486-52af-4d04-8648-24c91ec0986d"},"source":["epochs = 3\n","batch_size = 8\n","model = create_model(learning_rate=3e-5, decay=1e-6)\n","# model.summary()\n","\n","model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,\n","      validation_data=(x_eval, y_eval))\n","\n","model.save(path+\"save/model.h5\")\n","\n","print(\"TRAINING PROCESS DONE!!!!!!!!!!!!!\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/3\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n","3025/3714 [=======================>......] - ETA: 15:21 - loss: 3.8113 - activation_2_loss: 1.9284 - activation_3_loss: 1.8829 - activation_2_accuracy: 0.4684 - activation_3_accuracy: 0.4755"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IVVoEcfiuY3x"},"source":["# Test model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7Od3Yq4EzIE","executionInfo":{"elapsed":49817,"status":"ok","timestamp":1626150503986,"user":{"displayName":"漏气了","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64","userId":"08159925913877395493"},"user_tz":420},"outputId":"002ccff9-b1f7-48f6-f4a8-8b4155fa6cf3"},"source":["model =keras.models.load_model(path+\"save/model.h5\")\n","\n","model.evaluate(x_test,y_test,batch_size=8)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","74/74 [==============================] - 25s 269ms/step - loss: 14.5579 - activation_loss: 7.2559 - activation_1_loss: 7.3020 - activation_accuracy: 0.2445 - activation_1_accuracy: 0.2479\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[14.557933807373047,\n"," 7.255949974060059,\n"," 7.301981449127197,\n"," 0.24448217451572418,\n"," 0.24787776172161102]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"Bl4OgmKDPFT0"},"source":["Test model with custom data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4Rn_qG5Yu3s","executionInfo":{"elapsed":3343,"status":"ok","timestamp":1626150514854,"user":{"displayName":"漏气了","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64","userId":"08159925913877395493"},"user_tz":420},"outputId":"4ddcc927-f0b6-4e07-ca1f-77820c88679e"},"source":["test_data = {\"data\":\n","    [\n","        {\"title\": \"Project Apollo\",\n","         \"paragraphs\": [\n","             {\n","                \"context\": \"The pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at 1 m\\u00b7s\\u22122 when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sth\\u00e8ne, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf.\",\n","                 \"qas\": [\n","                         {\"question\": \"What is the metric term less used than the Newton?\", \"id\": \"5737aafd1c456719005744fb\", \n","                          \"answers\": [{\"text\": \"kilogram-force\", \"answer_start\": 82}, \n","                                      {\"text\": \"pound-force\", \"answer_start\": 4}, \n","                                      {\"text\": \"kilogram-force (kgf)\", \"answer_start\": 82}, \n","                                      {\"text\": \"kilogram-force\", \"answer_start\": 82}, \n","                                      {\"text\": \"the kilogram-force (\", \"answer_start\": 78}]}, \n","                         {\"question\": \"What is the kilogram-force sometimes reffered to as?\", \"id\": \"5737aafd1c456719005744fc\", \n","                          \"answers\": [{\"text\": \"kilopond\", \"answer_start\": 114}, {\"text\": \"kilopond\", \"answer_start\": 114}, {\"text\": \"kilopond\", \"answer_start\": 114}, {\"text\": \"kilopond\", \"answer_start\": 114}, {\"text\": \"kilopond\", \"answer_start\": 114}]}, \n","                         {\"question\": \"What is a very seldom used unit of mass in the metric system?\", \"id\": \"5737aafd1c456719005744fd\", \n","                          \"answers\": [{\"text\": \"slug\", \"answer_start\": 274}, {\"text\": \"metric slug\", \"answer_start\": 267}, {\"text\": \"metric slug\", \"answer_start\": 267}, {\"text\": \"metric slug\", \"answer_start\": 267}, {\"text\": \"the metric slug\", \"answer_start\": 263}]}, \n","                         {\"question\": \"What seldom used term of a unit of force equal to 1000 pound s of force?\", \"id\": \"5737aafd1c456719005744fe\", \n","                          \"answers\": [{\"text\": \"kip\", \"answer_start\": 712}, {\"text\": \"kip\", \"answer_start\": 712}, {\"text\": \"kip\", \"answer_start\": 712}, {\"text\": \"kip\", \"answer_start\": 712}, {\"text\": \"kip\", \"answer_start\": 712}]}, \n","                         {\"question\": \"What is the seldom used force unit equal to one thousand newtons?\", \"id\": \"5737aafd1c456719005744ff\", \n","                          \"answers\": [{\"text\": \"sth\\u00e8ne\", \"answer_start\": 665}, {\"text\": \"sth\\u00e8ne\", \"answer_start\": 665}, {\"text\": \"sth\\u00e8ne\", \"answer_start\": 665}, {\"text\": \"sth\\u00e8ne\", \"answer_start\": 665}, {\"text\": \"sth\\u00e8ne\", \"answer_start\": 665}]}, \n","]}]}]}\n","\n","\n","test_samples = convert_squad2data(test_data)\n","x_test, _ = create_input_dataset(test_samples)\n","\n","pred_start, pred_end = model.predict(x_test)\n","\n","for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n","    test_sample = test_samples[idx]\n","    offsets = test_sample.context_token_to_char\n","    start = np.argmax(start)\n","    end = np.argmax(end)\n","    pred_ans = None\n","    if start >= len(offsets):\n","        continue\n","    pred_char_start = offsets[start][0]\n","    if end < len(offsets):\n","        pred_char_end = offsets[end][1]\n","        pred_ans = test_sample.context[pred_char_start:pred_char_end]\n","    else:\n","        pred_ans = test_sample.context[pred_char_start:]\n","\n","    print(\"Question: \" + test_sample.question)\n","    print(\"Predict answer: \" + pred_ans)\n","    if test_samples[idx].answer_text:\n","        print(\"Correct answer: \" + test_samples[idx].answer_text)\n","        for a in test_samples[idx].all_answers:\n","            print(\"All possible answer: \" + a)\n","    print(\"+++++++++++++++++++++++++++++++++++++++++\\n\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Question: What is the metric term less used than the Newton?\n","Predict answer: The pound-force\n","Correct answer: kilogram-force\n","All possible answer: kilogram-force\n","All possible answer: pound-force\n","All possible answer: kilogram-force (kgf)\n","All possible answer: kilogram-force\n","All possible answer: the kilogram-force (\n","+++++++++++++++++++++++++++++++++++++++++\n","\n","Question: What is the kilogram-force sometimes reffered to as?\n","Predict answer: the kilogram-force (kgf) (sometimes kilopond\n","Correct answer: kilopond\n","All possible answer: kilopond\n","All possible answer: kilopond\n","All possible answer: kilopond\n","All possible answer: kilopond\n","All possible answer: kilopond\n","+++++++++++++++++++++++++++++++++++++++++\n","\n","Question: What is a very seldom used unit of mass in the metric system?\n","Predict answer: the metric slug\n","Correct answer: slug\n","All possible answer: slug\n","All possible answer: metric slug\n","All possible answer: metric slug\n","All possible answer: metric slug\n","All possible answer: the metric slug\n","+++++++++++++++++++++++++++++++++++++++++\n","\n","Question: What seldom used term of a unit of force equal to 1000 pound s of force?\n","Predict answer: the newton\n","Correct answer: kip\n","All possible answer: kip\n","All possible answer: kip\n","All possible answer: kip\n","All possible answer: kip\n","All possible answer: kip\n","+++++++++++++++++++++++++++++++++++++++++\n","\n","Question: What is the seldom used force unit equal to one thousand newtons?\n","Predict answer: The pound-force\n","Correct answer: sthène\n","All possible answer: sthène\n","All possible answer: sthène\n","All possible answer: sthène\n","All possible answer: sthène\n","All possible answer: sthène\n","+++++++++++++++++++++++++++++++++++++++++\n","\n"],"name":"stdout"}]}]}