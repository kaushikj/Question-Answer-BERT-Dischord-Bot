{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 21576,
     "status": "ok",
     "timestamp": 1626384976018,
     "user": {
      "displayName": "漏气了",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64",
      "userId": "08159925913877395493"
     },
     "user_tz": 420
    },
    "id": "5UoR44og9e0u"
   },
   "outputs": [],
   "source": [
    "# set up the environment\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
    "\n",
    "import findspark \n",
    "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")# SPARK_HOME\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16262,
     "status": "ok",
     "timestamp": 1626384875238,
     "user": {
      "displayName": "漏气了",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64",
      "userId": "08159925913877395493"
     },
     "user_tz": 420
    },
    "id": "wb45dB5d99oi",
    "outputId": "71882dc2-dfa7-44c7-95ea-cf60278e64a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1733,
     "status": "ok",
     "timestamp": 1626384936044,
     "user": {
      "displayName": "漏气了",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64",
      "userId": "08159925913877395493"
     },
     "user_tz": 420
    },
    "id": "Dw1iJV5m-Qtf",
    "outputId": "a9eda522-b828-4e4c-d2f8-cfa222fd17cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myBjt7JEAb1I"
   },
   "source": [
    "# Build the index using a document collection\n",
    "\n",
    "Create an RDD from a text file\n",
    "\n",
    "Each line of the text file becomes an element of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1135,
     "status": "ok",
     "timestamp": 1626384984456,
     "user": {
      "displayName": "漏气了",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64",
      "userId": "08159925913877395493"
     },
     "user_tz": 420
    },
    "id": "tkJ9ukGqAfk5"
   },
   "outputs": [],
   "source": [
    "# wholeTextFiles generates an RDD of pair values, \n",
    "# where the key is the full path of each file, the value is the content of each file\n",
    "# input = sc.wholeTextFiles(\"/content/drive/My\\ Drive/input_docs\");\n",
    "# return [(wholepath, wholetxt), ...]\n",
    "input = sc.wholeTextFiles(\"/content/drive/My Drive/input_doc_all/\");\n",
    "\n",
    "# Now we strip the prefix of filenames and leave only the basename. \n",
    "# e.g. 'file:/content/drive/My Drive/Colab Notebooks/data_spark/input_docs/3.html'\n",
    "# becomes '3.html' \n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "input2 = input.map(lambda x: (int(os.path.basename(x[0]).split(\".\")[0]), x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275914,
     "status": "ok",
     "timestamp": 1626385266852,
     "user": {
      "displayName": "漏气了",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64",
      "userId": "08159925913877395493"
     },
     "user_tz": 420
    },
    "id": "KlaFhW623zQG",
    "outputId": "2ad4a3d2-f962-4345-a0cc-5a052b73af47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16807, 'What province is Galkayo located in?')]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S7OTmFcHcT-"
   },
   "source": [
    "# Create RDD of （word，（docid，frqc，tf））"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef3ghmOjHhQj"
   },
   "outputs": [],
   "source": [
    "# Doc to wordlist function\n",
    "# The output will be a list of tuples such as \n",
    "# (\"apple\", (3,10,10/20)), \n",
    "# where 3 is docid, \n",
    "# 10 is frequency of \"apple\" in this doc, \n",
    "# 20 is maxf in in this doc.\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# for a given doc return a list of tuples of the form (w, (docid, freq, freq/maxfreq))\n",
    "def dw(docid, htmltext):\n",
    "  \n",
    "  # Make all words lowercase\n",
    "  cleantext = BeautifulSoup(htmltext).get_text().lower()\n",
    "  tokenizedText = re.findall(r\"\\b[a-z]+\\b\",cleantext)\n",
    "\n",
    "  # remove all the stop words, return a list of words\n",
    "  filteredToken = [w for w in tokenizedText \n",
    "                   if not w in stop_words]\n",
    "  # couter words, return a dic\n",
    "  tokenCnt = Counter(filteredToken)\n",
    "\n",
    "  maxf = tokenCnt.most_common(1)[0][1]\n",
    "\n",
    "  # create a nwe dic\n",
    "  wdft = dict()\n",
    "\n",
    "  for t in filteredToken:\n",
    "    # frqc returns the count of the word\n",
    "    frqc = tokenCnt[t]\n",
    "\n",
    "    #return a tuple corresponding the word\n",
    "    wdft[t] = (docid,frqc,frqc/maxf)\n",
    "  # return a dic corresponding the word with docid,frqc,frqc/maxf\n",
    "  return wdft\n",
    "word_docid_freq_tf = input2.flatMap(lambda x: [(t,dw(x[0],x[1])[t]) for t in dw(x[0],x[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKSsHyrgsoEa"
   },
   "source": [
    "Now create an RDD as follows \n",
    "\n",
    "e.g. (word, [(did1,freq1,tf1), (did2,freq2,tf2), ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bF5HjkbsrkB"
   },
   "outputs": [],
   "source": [
    "word_docid_freq_tf_2=word_docid_freq_tf.groupByKey().map(lambda x : (x[0], list(x[1])))\n",
    "print(word_docid_freq_tf_2.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDvi9Uy7uDQz"
   },
   "outputs": [],
   "source": [
    "# We easily obtain idf as 1/len(postinglist_tf)\n",
    "word_docid_freq_tfidf= word_docid_freq_tf_2.map(lambda x : (x[0], list(map(lambda y: (y[0], y[1], y[2]/len(x[1])), x[1] ))))\n",
    "print(word_docid_freq_tfidf.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1316,
     "status": "ok",
     "timestamp": 1626303752064,
     "user": {
      "displayName": "漏气了",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64",
      "userId": "08159925913877395493"
     },
     "user_tz": 420
    },
    "id": "zsShT7Ns2cJz",
    "outputId": "31746a83-90f9-4a4b-dba2-1b9d5770640e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('solo', (15, 1, 0.058823529411764705)), ('solo', (10, 1, 0.058823529411764705))]\n",
      "[(15, (1, 0.0034602076124567475)), (10, (1, 0.0034602076124567475))]\n",
      "[(10, (1, 0.0038591326840581855)), (14, (1, 0.0035248396701107894))]\n"
     ]
    }
   ],
   "source": [
    "# Now, we would like to obtain the magnitude of each doc.\n",
    "# First, produce (did, ( freq,tfidf)) for each word of doc did; \n",
    "# We do don't need the word itself, just its (freq,tfidf). \n",
    "# Then, do reduceByKey on these tuples and obtain maxfreq and \n",
    "# magnitude (squared) for each document. \n",
    "import math;\n",
    "#TODO\n",
    "# RDD of (did,(freq,tfidf)) tuples\n",
    "did_freq_tfidf=word_docid_freq_tfidf.flatMapValues(lambda x:x)\n",
    "print(did_freq_tfidf.take(2))\n",
    "did_freq_tfidf=did_freq_tfidf.map(lambda x:(x[1][0],(x[1][1],x[1][2]*x[1][2])))\n",
    "print(did_freq_tfidf.take(2))\n",
    "doc_maxf_mag=did_freq_tfidf.reduceByKey(lambda x,y:(max(x[0],y[0]),x[1]+y[1]))\n",
    "print(doc_maxf_mag.take(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW-Q-BFgGgUE"
   },
   "source": [
    "# Save doc2mag and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLPW9UcnCiK9"
   },
   "outputs": [],
   "source": [
    "!rm -rf inv_idx\n",
    "word_docid_freq_tfidf.repartition(1).saveAsTextFile(\"inv_idx\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNf2Zdl5CljT"
   },
   "outputs": [],
   "source": [
    "!rm -rf doc_mag\n",
    "doc_maxf_mag.repartition(1).saveAsTextFile(\"doc_mag\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1626304427261,
     "user": {
      "displayName": "漏气了",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64",
      "userId": "08159925913877395493"
     },
     "user_tz": 420
    },
    "id": "FibZxYzsDnKr",
    "outputId": "b2536b6d-0e0a-416f-a85a-b4a69ea209bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2792\n",
      "-rw-r--r-- 1 root root 2855593 Jul 14 23:12 part-00000\n",
      "-rw-r--r-- 1 root root       0 Jul 14 23:12 _SUCCESS\n",
      "12136 /content/drive/My Drive/inv_idx.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lrt inv_idx\n",
    "#!head inv_idx/part-00001\n",
    "#!wc -l inv_idx/part-00000\n",
    "#!wc -l inv_idx/part-00001\n",
    "!cat inv_idx/part-00000 > /content/drive/My\\ Drive/inv_idx.txt\n",
    "!wc -l /content/drive/My\\ Drive/inv_idx.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 898,
     "status": "ok",
     "timestamp": 1626304441307,
     "user": {
      "displayName": "漏气了",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9OAcNICyclgkTml2mJWyIV5_1NRfibRHA2V_73A=s64",
      "userId": "08159925913877395493"
     },
     "user_tz": 420
    },
    "id": "rhoNnQadD4ji",
    "outputId": "e8596449-0bd5-4290-a46b-ed42a323aa78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 580\n",
      "-rw-r--r-- 1 root root 591624 Jul 14 23:12 part-00000\n",
      "-rw-r--r-- 1 root root      0 Jul 14 23:12 _SUCCESS\n",
      "(10, (1, 0.0038591326840581855))\n",
      "(14, (1, 0.0035248396701107894))\n",
      "(2, (1, 0.006177163001462405))\n",
      "(12, (1, 0.006342785118225462))\n",
      "(130, (1, 0.005674244508596622))\n",
      "(134, (1, 0.008530274174313483))\n",
      "(4764, (1, 0.004658267184246462))\n",
      "(68, (1, 0.002070250129949385))\n",
      "(102, (1, 0.06403653109524758))\n",
      "(104, (1, 0.06804660087133721))\n",
      "17807 doc_mag/part-00000\n",
      "wc: doc_mag/part-00001: No such file or directory\n",
      "17807 /content/drive/My Drive/doc_mag.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lrt doc_mag\n",
    "!head doc_mag/part-00000\n",
    "!wc -l doc_mag/part-00000\n",
    "!wc -l doc_mag/part-00001\n",
    "!cat doc_mag/part-00000 > /content/drive/My\\ Drive/doc_mag.txt\n",
    "!wc -l /content/drive/My\\ Drive/doc_mag.txt"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN+kbL7kXg7n6w8yF+bJk61",
   "collapsed_sections": [],
   "name": "serach_engine.ipynb",
   "provenance": [
    {
     "file_id": "190H8S-wwJruLKduQn0gdG-rNonpUbnPN",
     "timestamp": 1626296813586
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
